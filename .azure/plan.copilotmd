# Azure Deployment Plan for documind-ai-foundry Project

## **Goal**
Deploy a hybrid AI infrastructure with Azure AI Foundry workspace containing GPT-4o, GPT-4o-mini, and text-embedding-3-large models for real-time information access and cloud AI capabilities integrated with local GPU models.

## **Project Information**
AppName: documind-ai-foundry
- **Technology Stack**: Python 3.11+ with .NET 8 SDK, Azure AI Foundry SDK, OpenAI SDK
- **Application Type**: Hybrid AI inference system with local GPU models (Ollama + llama.cpp) and cloud AI models
- **Containerization**: Python container apps for AI services with multi-stage builds
- **Dependencies**: Azure AI Foundry workspace, OpenAI model deployments, GPU-optimized local inference
- **Hosting Recommendation**: Azure Container Apps for scalable AI service hosting with Azure AI Foundry for model hosting

## **Azure Resources Architecture**
```mermaid
graph TB
    Client[Client Applications] --> LB[Load Balancer]
    LB --> ACA[Azure Container Apps]
    ACA --> AIFoundry[Azure AI Foundry Workspace]
    ACA --> KV[Key Vault]
    ACA --> CR[Container Registry]
    ACA --> AI[Application Insights]
    
    AIFoundry --> GPT4O[GPT-4o Deployment]
    AIFoundry --> GPT4OMini[GPT-4o-mini Deployment] 
    AIFoundry --> Embedding[Text-Embedding-3-Large]
    
    ACA --> LocalGPU[Local GPU Models]
    LocalGPU --> Ollama[Ollama Models]
    LocalGPU --> LlamaCPP[llama.cpp Models]
    
    subgraph "AI Model Routing"
        Router[Smart Router] --> Local[Local GPU Tier]
        Router --> Cloud[Cloud AI Tier]
    end
```

Data flow:
- The container app gets its image from the Azure Container Registry
- The smart router decides between local GPU models and Azure AI Foundry models based on complexity and cost
- Azure AI Foundry provides GPT-4o for complex reasoning and real-time information
- Local GPU models handle coding, batch processing, and cost-sensitive workloads
- Application Insights monitors performance and usage across hybrid infrastructure

## **Recommended Azure Resources**

Recommended App service hosting the project:
- Application documind-ai-foundry
  - Hosting Service Type: Azure Container Apps
  - SKU: Consumption plan with GPU-enabled local compute, auto-scaling 0-10 instances
  - Configuration:
    - language: python
    - dockerFilePath: ./src/python/Dockerfile
    - dockerContext: ./src/python
    - Environment Variables: 
      - AZURE_AI_FOUNDRY_ENDPOINT
      - AZURE_AI_FOUNDRY_KEY_VAULT_NAME
      - OLLAMA_ENDPOINT=http://localhost:11434
      - LLAMACPP_ENDPOINT=http://localhost:8081
      - GPU_MEMORY_LIMIT=24GB
  - Dependencies Resource
    - Azure AI Foundry Workspace
    - SKU: Standard S0 (Pay-per-use with reserved capacity)
    - Service Type: Azure AI Foundry with OpenAI model deployments
    - Connection Type: Managed identity with Key Vault secrets
    - Environment Variables:
      - AZURE_OPENAI_GPT4O_DEPLOYMENT_NAME
      - AZURE_OPENAI_GPT4O_MINI_DEPLOYMENT_NAME  
      - AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME

Recommended Supporting Services:
- Application Insights: Monitor hybrid AI performance and costs
- Log Analytics Workspace: Centralized logging for all AI services
- Key Vault: Store Azure AI Foundry connection strings and API keys securely
- Container Registry: Store Python AI service container images
- Storage Account: Model cache and inference logs storage

Recommended Security Configurations:
- User managed identity: Assigned to the container app for Azure AI Foundry access
- AcrPull role assignment: User managed identity has AcrPull role (7f951dda-4ed3-4680-a7ca-43fe172d538d) for container registry
- Cognitive Services User role: For Azure AI Foundry model access
- Key Vault Secrets User role: For secure credential management

## **Execution Step**

1. Provision Azure Infrastructure And Deploy the Application:
   1. Based on following required Azure resources in plan, get the IaC rules from the tool `iac-rules-get`
   2. Generate IaC (bicep files) for required azure resources based on the plan
   3. Pre-check: use `get_errors` tool to check generated Bicep grammar errors. Fix the errors if exist
   4. Run the AZD command `azd up` to provision the resources and confirm each resource is created or already exists
   5. Check the deployment output to ensure the resources are provisioned successfully
   6. Check the application log with tool `azd-app-log-get` to ensure the services are running

2. Summary:
   1. Summarize the deployment result and save to '.azure/summary.copilotmd'. It should list all changes deployment files and brief description of each file. Then have a diagram showing the provisioned azure resource.