#!/usr/bin/env python3
"""
Web Crawler Demo for DocuMind
Demonstrates crawling Python docs, Stack Overflow, and Microsoft docs
"""
import asyncio
import sys
import os
sys.path.append('/home/dinesh/documind-engineering')

async def demo_crawler_urls():
    """Demo the crawler URL generation without actual crawling."""
    print("ğŸ•·ï¸  DocuMind Web Crawler Demo")
    print("=" * 50)

    # Import the crawler classes
    from src.python.services.rag_api.crawlers import (
        PythonDocsCrawler,
        StackOverflowCrawler,
        MicrosoftDocsCrawler,
        CrawlConfig
    )

    print("ğŸ“‹ Target Sites and URLs:")
    print()

    # Python Documentation URLs
    print("ğŸ Python Documentation:")
    python_crawler = PythonDocsCrawler()
    python_urls = python_crawler.get_python_docs_urls()[:8]
    for i, url in enumerate(python_urls, 1):
        print(f"  {i:2d}. {url}")

    print()

    # Stack Overflow URLs
    print("ğŸ“š Stack Overflow (Python topics):")
    so_crawler = StackOverflowCrawler()
    so_urls = so_crawler.get_python_stackoverflow_urls()[:6]
    for i, url in enumerate(so_urls, 1):
        print(f"  {i:2d}. {url}")

    print()

    # Microsoft Documentation URLs
    print("ğŸ¢ Microsoft Documentation:")
    ms_crawler = MicrosoftDocsCrawler()
    ms_urls = ms_crawler.get_microsoft_docs_urls()[:6]
    for i, url in enumerate(ms_urls, 1):
        print(f"  {i:2d}. {url}")

    print()
    print("ğŸ”§ Crawler Configuration Options:")
    config = CrawlConfig()
    print(f"  â€¢ Max pages per job: {config.max_pages}")
    print(f"  â€¢ Delay between requests: {config.delay_seconds}s")
    print(f"  â€¢ Max concurrent requests: {config.max_concurrent}")
    print(f"  â€¢ Content length limits: {config.min_content_length}-{config.max_content_length} chars")

    print()
    print("ğŸ¯ Domain Classification:")
    print("  â€¢ Technical: Python docs, Stack Overflow, Microsoft docs")
    print("  â€¢ Finance: Bloomberg, Reuters, Investopedia")
    print("  â€¢ Medical: PubMed, NIH, medical journals")
    print("  â€¢ Legal: Court documents, legal databases")
    print("  â€¢ Education: University sites, online courses")

    print()
    print("ğŸ“Š Integration with Vector Database:")
    print("  1. ğŸ•·ï¸  Crawl websites â†’ Extract content")
    print("  2. ğŸ” Domain classification â†’ Categorize content")
    print("  3. ğŸ“ Text chunking â†’ Split into manageable pieces")
    print("  4. ğŸ§® BGE-M3 embedding â†’ Convert to vectors")
    print("  5. ğŸ’¾ Qdrant storage â†’ Index in vector database")
    print("  6. ğŸ” Enhanced retrieval â†’ Domain-aware search")

if __name__ == "__main__":
    asyncio.run(demo_crawler_urls())
